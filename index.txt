1:"$Sreact.fragment"
2:I[4079,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-6a1014f1c98d8ca9.js","177","static/chunks/app/layout-0c3a73cb0fdb0a79.js"],"default"]
3:I[7558,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-6a1014f1c98d8ca9.js","177","static/chunks/app/layout-0c3a73cb0fdb0a79.js"],"ThemeProvider"]
4:I[9994,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-6a1014f1c98d8ca9.js","177","static/chunks/app/layout-0c3a73cb0fdb0a79.js"],"default"]
5:I[9766,[],""]
6:I[8924,[],""]
c:I[7150,[],""]
:HL["/_next/static/css/4d2005859cd7271e.css","style"]
0:{"P":null,"b":"N2uLGK7wv3XsbgogGx8py","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/4d2005859cd7271e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":[["$","$L2",null,{}],["$","$L3",null,{"children":[["$","$L4",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"üìë Publications","type":"page","target":"publications","href":"/publications"},{"title":"üêæ Supervision","type":"page","target":"supervision","href":"/supervision"},{"title":"üéì Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"üëú Services","type":"page","target":"services","href":"/services"},{"target":"talk","href":"/talk","type":"page","title":"üéØTalk"},{"target":"old_news","href":"/old_news","type":"page","title":"Old News"}],"siteTitle":"Qianli Wang","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L7","$L8"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L9"]}]]}]]}]]}],{"children":["__PAGE__","$La",{},null,false]},null,false],"$Lb",false]],"m":"$undefined","G":["$c",[]],"s":false,"S":true}
d:I[7923,["247","static/chunks/247-e0831dccaa8864d2.js","619","static/chunks/619-3ba632d834111881.js","918","static/chunks/918-6a1014f1c98d8ca9.js","177","static/chunks/app/layout-0c3a73cb0fdb0a79.js"],"default"]
e:I[1251,["247","static/chunks/247-e0831dccaa8864d2.js","796","static/chunks/796-b02d798fccb967eb.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-1f11213aa4622d10.js","974","static/chunks/app/page-465ff8df270af4f0.js"],"default"]
f:I[470,["247","static/chunks/247-e0831dccaa8864d2.js","796","static/chunks/796-b02d798fccb967eb.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-1f11213aa4622d10.js","974","static/chunks/app/page-465ff8df270af4f0.js"],"default"]
11:I[2597,["247","static/chunks/247-e0831dccaa8864d2.js","796","static/chunks/796-b02d798fccb967eb.js","619","static/chunks/619-3ba632d834111881.js","681","static/chunks/681-1f11213aa4622d10.js","974","static/chunks/app/page-465ff8df270af4f0.js"],"default"]
18:I[4431,[],"ViewportBoundary"]
1a:I[4431,[],"MetadataBoundary"]
1b:"$Sreact.suspense"
7:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
8:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
9:["$","$Ld",null,{"lastUpdated":"November 27, 2025"}]
10:T5f9,**2025-11-14:** One co-authored paper led by [Shengyun Si](https://shengyun-si.github.io/) on Paper2Figure benchmark üé® in submission. 

**2025-09-27:** üìù One paper about multilingual counterfactual evaluation in collaboration with LMU and Uni Marburg in submission.

**2025-09-01:** üå± Start my research stay in the [CopeNLU](https://www.copenlu.com/) group led by [Prof. Dr. Isabelle Augenstein](https://isabelleaugenstein.github.io/) in Denmarküßú. Thanks for hosting me and providing a nice starting point for our collaborations! 

**2025-08-21:** üí•Delighted to share that (1) _Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems_ accepted at _EMNLP 2025 Findings_; (2) _Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals_ accepted at _INLG 2025_. See you in Hanoi, Vietnam üáªüá≥ and Suzhou, China üá®üá≥! ‚úàÔ∏è 

**2025-05-19:** üí´ Three first-authored papers in submission about (1) _multilingual explanation request parsing and custom input extraction in ConvXAI_; (2) _counterfactual validity evaluation for counterfactual data augmentation_; (3) _how does quantization affect model explainability and interpretability_.

**2025-05-15:** The 4th first-authored paper titled _FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation_ accepted at _ACL 2025 Findings_. See you in Vienna, Austria üá¶üáπ! üçª


[News Archive](old_news/)12:T58a,Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming three state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF‚Äôs core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an importantfinding for future research in this direction.13:T8c2,@inproceedings{wang-etal-2025-fitcf,
  title = {{F}it{CF}: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation},
  author = {Wang, Qianli  and
      Feldhus, Nils  and
      Ostermann, Simon  and
      Villa-Arenas, Luis Felipe  and
      M√∂ller, Sebastian  and
      Schmitt, Vera},
  editor = {Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher},
  booktitle = {Findings of the Association for Computational Linguistics: ACL},
  month = {jul},
  year = {2025},
  address = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.findings-acl.64/},
  doi = {10.18653/v1/2025.findings-acl.64},
  pages = {1176--1191},
  ISBN = {979-8-89176-256-5},
  abstract = {Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming three state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF‚Äôs core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an importantfinding for future research in this direction.}
}14:T55e,Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.15:T87a,@inproceedings{wang-etal-2025-cross,
  title = {Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem},
  author = {Wang, Qianli  and
      Anikina, Tatiana  and
      Feldhus, Nils  and
      Ostermann, Simon  and
      M√∂ller, Sebastian  and
      Schmitt, Vera},
  editor = {Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  month = {jan},
  year = {2025},
  address = {Abu Dhabi, UAE},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.coling-main.77/},
  pages = {1150--1167},
  abstract = {Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.}
}a:["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$Le",null,{"author":{"name":"Qianli Wang","title":"PhD Student","institution":"Technische Universit√§t Berlin","avatar":"/avatar.jpg"},"social":{"email":"qianli.wang@tu-berlin.de","location":"Berlin, Germany","location_url":"https://maps.google.com","location_details":["Room 2.052, MAR Building","Marchstra√üe 23, 10587 Berlin, Germany"],"google_scholar":"https://scholar.google.com/citations?user=dKmUzp4AAAAJ","github":"https://github.com/qiaw99","linkedin":"https://www.linkedin.com/in/qianliwang/"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["post-hoc explanations (counterfactual example, free-text rationale)","interactive conversational XAI systems","mechanistic interpretability","explanation faithfulness","multilinguality","evaluation"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$Lf","about",{"content":"Hiüëã, I'm Qianli Wang and work as a NLP researcher dedicated to advancing the interpretability and explainability of LLMs at [XplainNLP group](https://www.tu.berlin/qu/forschung/forschungsgruppen/xplainlp), [Quality and Usability Lab](https://www.tu.berlin/qu), [Technische Universit√§t Berlin](https://www.tu.berlin/) under the supervision of [Prof. Dr.-Ing. Sebastian M√∂ller](https://www.tu.berlin/qu/ueber-uns/leitung).","title":"About"}],["$","$Lf","news",{"content":"$10","title":"News"}],["$","$L11","featured_publications",{"publications":[{"id":"wang-etal-2025-fitcf","title":"FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation","authors":[{"name":"Qianli Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Nils Feldhus","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Simon Ostermann","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Luis Felipe Villa-Arenas","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sebastian M√∂ller","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Vera Schmitt","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$a:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Findings of the Association for Computational Linguistics: ACL","pages":"1176--1191","doi":"10.18653/v1/2025.findings-acl.64","url":"https://aclanthology.org/2025.findings-acl.64/","code":"https://github.com/qiaw99/FitCF","abstract":"$12","description":"","selected":true,"preview":"fitcf.png","bibtex":"$13"},{"id":"wang-etal-2025-cross","title":"Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem","authors":[{"name":"Qianli Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Tatiana Anikina","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Nils Feldhus","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Simon Ostermann","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sebastian M√∂ller","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Vera Schmitt","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"1","type":"conference","status":"published","tags":[],"keywords":"$a:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 31st International Conference on Computational Linguistics","pages":"1150--1167","url":"https://aclanthology.org/2025.coling-main.77/","code":"https://github.com/qiaw99/Cross-Refine","abstract":"$14","description":"","selected":true,"preview":"cross.png","bibtex":"$15"}],"title":"Selected Publications","enableOnePageMode":false}],"$L16"],false,false,false]}]]}]]}]}],null,"$L17"]}]
b:["$","$1","h",{"children":[null,[["$","$L18",null,{"children":"$L19"}],null],["$","$L1a",null,{"children":["$","div",null,{"hidden":true,"children":["$","$1b",null,{"fallback":null,"children":"$L1c"}]}]}]]}]
1e:I[4431,[],"OutletBoundary"]
20:I[5278,[],"AsyncMetadataOutlet"]
1d:T648,# Education Background üë®‚Äçüéì

2024.07-ongoing: Computer Science, Ph.D., Technische Universit√§t Berlin. Supervised by Prof. Dr. Sebastian M√∂ller.

2025.09: Visiting PhD Student, University of Copenhagen. Supervised by Prof. Dr. Isabelle Augenstein.

2021.10-2023.10: Computer Science, M.Sc., Technische Universit√§t Berlin. Supervised by [Dr.-Ing. Nils Feldhus](https://nfelnlp.github.io/) and [Dr.-Ing. Leonhard Hennig](https://www.dfki.de/web/ueber-uns/mitarbeiter/person/lehe02): _A Singular LLM is all you need for dialogue-based explanation regarding NLP tasks_.

2018.10-2021.09: Computer Science, B.Sc., Freie Universit√§t Berlin. Supervised by [Prof. Dr. L√°szl√≥ Kozma](https://www.mi.fu-berlin.de/inf/groups/ag-ti/members/professoren/Kozma_Laszlo.html): _Understanding and implementation of the algorithm for three-coloring in triangle-free planar graphs_.


# Jobs üßë‚Äçüíª
2024.07 - ongoing: NLP Researcher @ Quality and Usability Lab, Technische Universit√§t Berlin, Berlin, Germany; funded by [VERANDA](https://njctn.github.io/VERANDA/) project.

2024.07 - ongoing: Guest Researcher @ German Research Center for Artificial Intelligence (DFKI) - [SLT Group](https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/), Berlin, Germany.

2024.01 - 2024.05: Machine Learning Researcher @ Aptiv, Wuppertal, Germany.

2023.01 - 2024.01: Research Assistant @ German Research Center for Artificial Intelligence (DFKI) - [SLT Group](https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/), Berlin, Germany.

16:["$","$Lf","cv",{"content":"$1d","title":""}]
17:["$","$L1e",null,{"children":["$L1f",["$","$L20",null,{"promise":"$@21"}]]}]
19:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1f:null
22:I[622,[],"IconMark"]
21:{"metadata":[["$","title","0",{"children":"Qianli Wang"}],["$","meta","1",{"name":"description","content":"PhD student at Technische Universit√§t Berlin"}],["$","meta","2",{"name":"author","content":"Qianli Wang"}],["$","meta","3",{"name":"keywords","content":"Qianli Wang,PhD,Research,Technische Universit√§t Berlin"}],["$","meta","4",{"name":"creator","content":"Qianli Wang"}],["$","meta","5",{"name":"publisher","content":"Qianli Wang"}],["$","meta","6",{"property":"og:title","content":"Qianli Wang"}],["$","meta","7",{"property":"og:description","content":"PhD student at Technische Universit√§t Berlin"}],["$","meta","8",{"property":"og:site_name","content":"Qianli Wang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Qianli Wang"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at Technische Universit√§t Berlin"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L22","15",{}]],"error":null,"digest":"$undefined"}
1c:"$21:metadata"

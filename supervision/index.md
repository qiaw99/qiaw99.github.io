---
layout: single
classes: wide
author_profile: true
title: "Supervision"
---

## Thesis Supervision
### Ongoing
- [Jiaao Li](https://github.com/grayJiaaoLi) - **Master Thesis** - Title: _EffiCoC: Efficient Chain-of-Code with CodeLLMs_

- Alexandra Iakovleva  - **Bachelor Thesis** - Title: _XAI in Data Privacy and Data Anonymization: A Survey_

- [Fedor Splitt](https://github.com/fledor) - **Bachelor Thesis** - Title: _Counterfactual Explanation Enhancement via Self-Alignment_

### Completed
- Yilong Wang  - **Bachelor Thesis** - Title: _AIGC: Automatic Iterative Generation for Counterfactuals_

## Student Research Assistant Supervision
### Ongoing
- [Fedor Splitt](https://github.com/fledor) (2025/02/01-): intent recognition in ConvXAI and counterfactual data augmentation

### Completed
- [Yoana Tsoneva](https://github.com/ytsoneva24) (2025/01/16-2025/03/15): intent recognition in conversational XAI systems

## Open Topics
If you are interested in any topics mentioned below, please send me your CV, transcript of record, and a short motivation letter.

[//]: # (### 1. ToolFormer integration in conversational XAI systems)

[//]: # (As LLMs is getting more and more complex, the need for explaining black box model is growing. However, sometimes one-off explanations are not sufficient, e.g., due to ambiguity. Thus, we introduced interactive conversational XAI systems, which can provide multiple explainability methods in a dialogue manner. The code can be reused from below referenced papers &#40;https://github.com/DFKI-NLP/InterroLang&#41;, which can also give you an overview how such systems look like. The goal of this thesis is: try to integrate Toolformer into conversation XAI systems. When a specific explainability method is called by the user, the user question should be annotated by some certain patterns, with help of which corresponding tools should be invoked.)

[//]: # ()
[//]: # (References: <a href="https://aclanthology.org/2023.findings-emnlp.359/">Feldhus et al. &#40;2023&#41;</a>; <a href="https://aclanthology.org/2024.hcinlp-1.9/">Wang et al. &#40;2024&#41;</a>; <a href="https://openreview.net/pdf?id=Yacmpz84TH">Schick et al. &#40;2023&#41;</a>)

### 1. Development of new approaches for generating alteractuals
Counterfactuals refer to the edited input whose prediction after edition is different than that before edition. However, alterfactual is the opposition, which refers to altered version and the prediction does not change. Since alterfactual generation is less explored by the NLP communities, the goal of the thesis is to create a new approach to generate alterfactual.

References: <a href="https://arxiv.org/pdf/2405.05295">Mertes et al. (2024)</a>;  <a href="https://www.arxiv.org/pdf/2408.10528">Nguyen et al. (2024)</a>

### 2. Refining and Aligning Natural Language Explanations through Human-Guided Feedback
References: [Li et al. (2022)](https://aclanthology.org/2022.findings-acl.75/); [Hong et al. (2024)](https://aclanthology.org/2024.emnlp-main.626/); [Chen et al. (2024)](https://arxiv.org/abs/2412.08393)

### 3. How different quantization approaches affect feature importance
Quantization approaches can be divided into two categories: quantization-aware training and post-training quantization. In this thesis, we want to explore how different approaches affect feature importance, e.g. faithfulness.

Reference: [Gholami et al. (2022)](https://arxiv.org/abs/2103.13630); [Wan et al. (2024)](https://openreview.net/forum?id=bsCCJHbO8A); [Zhu et al. (2024)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482/A-Survey-on-Model-Compression-for-Large-Language);